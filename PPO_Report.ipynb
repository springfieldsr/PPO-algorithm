{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37264bit29c01369fd6a40ed94bf67a5c06978ff",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Report\n",
    "\n",
    "This project is to solve CartPole problem from Gym OpenAI. Following is sample code of PPO algorithm provided by https://morvanzhou.github.io/tutorials. My understanding of each line of sample code is in the inline comments, and feel free to check the source file.\n",
    "\n",
    "To summarize, PPO algorithm is a simplified version of TRPO algorithm. It shares part of the objective function of TRPO, which is J(θ)=E\\[r(θ)A^θold(s,a)\\]. However, to stablize the learning process, PPO imposes a constraint by forcing r(θ), which is the ratio between new and old policy, to be within a predefined range. So the objective function of PPO is as following: J(θ)=E\\[min(r(θ)A^θold(s,a), clip(r(θ),1−ϵ,1+ϵ)A^θold(s,a))\\]. \n",
    "\n",
    "Admittedly, there are other versions of PPO. The above clipping approach was proposed by OpenAI, which DeepMind proposes another method of KL divergence. The following sample code favors the former approach. \n",
    "\n",
    "Following figure is the result reward of the training, where 0 is the max reward each episode can obtain. The fluctuation is still huge even with desperately small learning rate. As PPO is still actor-critic, I suspect the same problem of the update process of critic, that it uses same network to estimate current state value and next state value. If you have any idea of how to deal with the inconvergence, please let me know.\n",
    "\n",
    "<img src=\"./reward.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is sample code with my inline comments.\n",
    "\n",
    "Part 1: hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max episodes\n",
    "EP_MAX = 1000\n",
    "# max iteration in each episode\n",
    "EP_LEN = 200\n",
    "# discount factor\n",
    "GAMMA = 0.9\n",
    "# learning rate of actor and critic\n",
    "A_LR = 0.00005\n",
    "C_LR = 0.0001\n",
    "# batch size\n",
    "BATCH = 32\n",
    "# update steps of actor and critic\n",
    "A_UPDATE_STEPS = 10\n",
    "C_UPDATE_STEPS = 10\n",
    "# dimension of states and action\n",
    "S_DIM, A_DIM = 3, 1\n",
    "# two reward functions proposed by DeepMind and OpenAI, respectively.\n",
    "# This code uses the clipping form by OpenAI\n",
    "METHOD = [\n",
    "    dict(name='kl_pen', kl_target=0.01, lam=0.5),   # KL penalty\n",
    "    dict(name='clip', epsilon=0.2),                 # Clipped surrogate objective, find this is better\n",
    "][1]        # choose the method for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: construction of PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        # placehold for states\n",
    "        self.tfs = tf.placeholder(tf.float32, [None, S_DIM], 'state')\n",
    "\n",
    "        # critic network\n",
    "        with tf.variable_scope('critic'):\n",
    "            # construct two fully connected layers for critic\n",
    "            l1 = tf.layers.dense(self.tfs, 100, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(l1, 1)\n",
    "            # placeholder for discounted reward\n",
    "            self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "            # Advantage = discounted_reward - estimated_value\n",
    "            self.advantage = self.tfdc_r - self.v\n",
    "            # loss of critic is the square of advantage\n",
    "            self.closs = tf.reduce_mean(tf.square(self.advantage))\n",
    "            # minimize the loss of critic \n",
    "            self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)\n",
    "\n",
    "        # actor network\n",
    "        # pi and oldpi are normal distributions\n",
    "        # pi_params and oldpi_params are their parameters\n",
    "        pi, pi_params = self._build_anet('pi', trainable=True)\n",
    "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)\n",
    "        # sample from the normal distribution and get actions later\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)       # choosing action\n",
    "        with tf.variable_scope('update_oldpi'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        # placeholder for action\n",
    "        self.tfa = tf.placeholder(tf.float32, [None, A_DIM], 'action')\n",
    "        # placeholder for advantage\n",
    "        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
    "        # construct loss function of actor\n",
    "        with tf.variable_scope('loss'):\n",
    "            with tf.variable_scope('surrogate'):\n",
    "                # ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))\n",
    "                # compute ratio of new policy and old policy\n",
    "                ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)\n",
    "                # compute the objective function of TRPO\n",
    "                surr = ratio * self.tfadv\n",
    "            if METHOD['name'] == 'kl_pen':\n",
    "                self.tflam = tf.placeholder(tf.float32, None, 'lambda')\n",
    "                kl = tf.distributions.kl_divergence(oldpi, pi)\n",
    "                self.kl_mean = tf.reduce_mean(kl)\n",
    "                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))\n",
    "            else:   # clipping method, find this is better\n",
    "                # choose the minimal between original TPRO and clipped TPRO\n",
    "                self.aloss = -tf.reduce_mean(tf.minimum(\n",
    "                    surr,\n",
    "                    tf.clip_by_value(ratio, 1.-METHOD['epsilon'], 1.+METHOD['epsilon'])*self.tfadv))\n",
    "        # train the actor network\n",
    "        with tf.variable_scope('atrain'):\n",
    "            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)\n",
    "\n",
    "        tf.summary.FileWriter(\"log/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # function to update both actor and critic networks\n",
    "    def update(self, s, a, r):\n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        # calculate advantage\n",
    "        adv = self.sess.run(self.advantage, {self.tfs: s, self.tfdc_r: r})\n",
    "        # adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful\n",
    "\n",
    "        # update actor\n",
    "        if METHOD['name'] == 'kl_pen':\n",
    "            for _ in range(A_UPDATE_STEPS):\n",
    "                _, kl = self.sess.run(\n",
    "                    [self.atrain_op, self.kl_mean],\n",
    "                    {self.tfs: s, self.tfa: a, self.tfadv: adv, self.tflam: METHOD['lam']})\n",
    "                if kl > 4*METHOD['kl_target']:  # this in in google's paper\n",
    "                    break\n",
    "            if kl < METHOD['kl_target'] / 1.5:  # adaptive lambda, this is in OpenAI's paper\n",
    "                METHOD['lam'] /= 2\n",
    "            elif kl > METHOD['kl_target'] * 1.5:\n",
    "                METHOD['lam'] *= 2\n",
    "            METHOD['lam'] = np.clip(METHOD['lam'], 1e-4, 10)    # sometimes explode, this clipping is my solution\n",
    "        else:   # clipping method, find this is better (OpenAI's paper)\n",
    "            [self.sess.run(self.atrain_op, {self.tfs: s, self.tfa: a, self.tfadv: adv}) for _ in range(A_UPDATE_STEPS)]\n",
    "\n",
    "        # update critic\n",
    "        [self.sess.run(self.ctrain_op, {self.tfs: s, self.tfdc_r: r}) for _ in range(C_UPDATE_STEPS)]\n",
    "\n",
    "    # function to build actor network, returns a normal distribution and its parameters\n",
    "    def _build_anet(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "            l1 = tf.layers.dense(self.tfs, 100, tf.nn.relu, trainable=trainable)\n",
    "            mu = 2 * tf.layers.dense(l1, A_DIM, tf.nn.tanh, trainable=trainable)\n",
    "            sigma = tf.layers.dense(l1, A_DIM, tf.nn.softplus, trainable=trainable)\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "\n",
    "    # function to choose action \n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        a = self.sess.run(self.sample_op, {self.tfs: s})[0]\n",
    "        return np.clip(a, -2, 2)\n",
    "\n",
    "    # function to estimate value\n",
    "    def get_v(self, s):\n",
    "        if s.ndim < 2: s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.v, {self.tfs: s})[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0').unwrapped\n",
    "ppo = PPO()\n",
    "all_ep_r = []\n",
    "\n",
    "for ep in range(EP_MAX):\n",
    "    # get initial state\n",
    "    s = env.reset()\n",
    "    # create buffer for state, action and reward\n",
    "    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "    # sum of reward each episode\n",
    "    ep_r = 0\n",
    "    for t in range(EP_LEN):    # in one episode\n",
    "        env.render()\n",
    "        # choose action based on state\n",
    "        a = ppo.choose_action(s)\n",
    "        # get next state, reward and end information\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        # append them to buffers\n",
    "        buffer_s.append(s)\n",
    "        buffer_a.append(a)\n",
    "        buffer_r.append((r+8)/8)    # normalize reward, find to be useful\n",
    "        # current state = next state\n",
    "        s = s_\n",
    "        # increase sum of reward\n",
    "        ep_r += r\n",
    "\n",
    "        # update ppo\n",
    "        if (t+1) % BATCH == 0 or t == EP_LEN-1:\n",
    "            v_s_ = ppo.get_v(s_)\n",
    "            discounted_r = []\n",
    "            for r in buffer_r[::-1]:\n",
    "                v_s_ = r + GAMMA * v_s_\n",
    "                discounted_r.append(v_s_)\n",
    "            discounted_r.reverse()\n",
    "\n",
    "            bs, ba, br = np.vstack(buffer_s), np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\n",
    "            buffer_s, buffer_a, buffer_r = [], [], []\n",
    "            ppo.update(bs, ba, br)\n",
    "    if ep == 0: all_ep_r.append(ep_r)\n",
    "    else: all_ep_r.append(all_ep_r[-1]*0.9 + ep_r*0.1)\n",
    "    print(\n",
    "        'Ep: %i' % ep,\n",
    "        \"|Ep_r: %i\" % ep_r,\n",
    "        (\"|Lam: %.4f\" % METHOD['lam']) if METHOD['name'] == 'kl_pen' else '',\n",
    "    )"
   ]
  }
 ]
}